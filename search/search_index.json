{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"EGAfetch","text":"<p>Fast, parallel, resumable download of data and metadata from the European Genome-phenome Archive (EGA).</p> <p>EGAfetch is a GO-based command-line tool an alternative to pyEGA3 with significantly faster downloads, automatic resume, and robust error handling.</p>"},{"location":"#why-egafetch","title":"Why EGAfetch?","text":"<p>Downloading large genomic datasets from EGA with pyEGA3 is slow, fragile, and requires manual inspection. EGAfetch solves this:</p> <ul> <li>Parallel downloads -- multiple files and multiple chunks per file downloaded simultaneously</li> <li>Automatic resume -- interrupted downloads pick up exactly where they stopped</li> <li>Checksum verification -- MD5/SHA256 verified after every file</li> <li>Token auto-refresh -- OAuth2 tokens refreshed transparently before expiry</li> <li>Retry with backoff -- exponential backoff with jitter on transient failures</li> <li>Metadata export -- download dataset metadata as TSV, CSV, or JSON (auto-fetched during download)</li> <li>Bandwidth throttling -- cap total bandwidth with <code>--max-bandwidth</code> for shared HPC networks</li> <li>Config file -- persist defaults in <code>~/.egafetch/config.yaml</code></li> <li>File filtering -- selectively download with <code>--include</code>/<code>--exclude</code> glob patterns</li> <li>Adaptive chunk sizing -- auto-tune chunk size based on throughput with <code>--adaptive-chunks</code></li> <li>Batch file input -- pass a text file with identifiers (one per line, <code>#</code> comments supported)</li> <li>MD5 sidecar files -- <code>.md5</code> checksum file written alongside each downloaded file</li> <li>Single binary -- no Python, no pip, no dependencies; works on HPC clusters</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code># Log in\negafetch auth login --cf credentials.json\n\n# Download an entire dataset with parallel chunked downloads\negafetch download EGAD00001001938 -o ./data\n\n# Interrupted? Just re-run -- it resumes automatically\negafetch download EGAD00001001938 -o ./data\n\n# Metadata is auto-fetched during download (when using --cf)\n# Or export independently:\negafetch metadata EGAD00001001938 --cf credentials.json\n</code></pre>"},{"location":"#at-a-glance","title":"At a Glance","text":"pyEGA3 EGAfetch Parallel files 1 Configurable (default 4) Parallel chunks 1 Configurable (default 8) Resume Limited Full (chunk-level, byte-precise) Token refresh Manual Automatic Bandwidth throttling No <code>--max-bandwidth</code> File filtering No <code>--include</code> / <code>--exclude</code> globs Adaptive chunks No <code>--adaptive-chunks</code> Persistent config No <code>~/.egafetch/config.yaml</code> Installation <code>pip install</code> Single binary Batch file input No Text file with identifiers MD5 sidecar files No <code>.md5</code> per file Metadata export No TSV / CSV / JSON (auto during download) <p>Ready to get started? Head to the Installation guide.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to EGAfetch are documented here.</p>"},{"location":"changelog/#v110-2026-02-16","title":"v1.1.0 (2026-02-16)","text":""},{"location":"changelog/#new-features","title":"New Features","text":"<ul> <li>Bandwidth throttling -- Global rate limit across all connections with <code>--max-bandwidth</code> (e.g., <code>100M</code>, <code>1G</code>). Uses <code>golang.org/x/time/rate.Limiter</code> shared across all goroutines.</li> <li>Adaptive chunk sizing -- <code>--adaptive-chunks</code> monitors throughput over a rolling window and auto-adjusts chunk sizes (8 MB -- 256 MB) based on connection speed.</li> <li>File filtering -- <code>--include</code> / <code>--exclude</code> glob patterns to selectively download files from a dataset (e.g., <code>--include \"*.bam\"</code>).</li> <li>Persistent configuration -- <code>~/.egafetch/config.yaml</code> for default settings (chunk size, parallelism, bandwidth, output dir, metadata format). CLI flags override config values.</li> <li>Identifier file input -- Pass a text file with one EGAD/EGAF identifier per line instead of listing IDs on the command line. Blank lines and <code>#</code> comments are supported.</li> <li>MD5 sidecar files -- <code>.md5</code> checksum file in standard <code>md5sum</code> format written alongside each downloaded file after verification.</li> <li>Automatic metadata download -- Dataset metadata (TSV/CSV/JSON + PEP) is fetched automatically after data download when using <code>--cf</code>. Control with <code>--no-metadata</code> and <code>--metadata-format</code>.</li> <li>Dataset details in <code>list</code> -- <code>egafetch list</code> now shows dataset title, description, and number of samples from the public metadata API.</li> </ul>"},{"location":"changelog/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Checksum verification -- <code>GetChecksum()</code> no longer falls back to the encrypted file's checksum (<code>Checksum</code> field). Only <code>PlainChecksum</code> and <code>UnencryptedChecksum</code> are used, preventing false mismatches when downloading in plain mode.</li> <li><code>.cip</code> extension stripping -- Output filenames now have the <code>.cip</code> extension stripped (e.g., <code>sample.bam.cip</code> becomes <code>sample.bam</code>), matching pyEGA3 behavior.</li> <li>Resume safety (HTTP 200 vs 206) -- When resuming a partial chunk, if the server returns HTTP 200 (ignoring the Range header) instead of 206 Partial Content, the existing <code>.part</code> file is now truncated instead of appended to, preventing data corruption.</li> </ul>"},{"location":"changelog/#other-changes","title":"Other Changes","text":"<ul> <li>Added <code>golang.org/x/time</code> and <code>gopkg.in/yaml.v3</code> dependencies.</li> <li>New <code>internal/config</code> package for YAML configuration.</li> <li>Adaptive chunk sizing logic in <code>internal/download/file.go</code> with batch dispatch and rechunking.</li> <li>Updated documentation across all pages.</li> </ul>"},{"location":"changelog/#v100","title":"v1.0.0","text":""},{"location":"changelog/#initial-release","title":"Initial Release","text":"<ul> <li>Two-level parallelism -- Configurable parallel files (default 4) and parallel chunks per file (default 8) for maximum throughput.</li> <li>Chunked downloads -- Files split into configurable chunks (default 64 MB) downloaded via HTTP Range requests.</li> <li>Byte-precise resume -- Interrupted downloads resume from the exact byte using persisted chunk state and append-mode writes.</li> <li>Atomic state persistence -- All state files written via temp file + fsync + rename to prevent corruption on crash.</li> <li>Automatic token refresh -- OAuth2 tokens refreshed 5 minutes before expiry using the refresh token.</li> <li>Checksum verification -- MD5/SHA256 verification after merge, with standalone <code>verify</code> command.</li> <li>Exponential backoff with jitter -- Up to 5 retries per chunk (1s base, 60s max) and 3 retries per file.</li> <li>Graceful shutdown -- SIGINT/SIGTERM saves state and preserves partial chunks for resume.</li> <li>Subcommands -- <code>auth login/status/logout</code>, <code>download</code>, <code>list</code>, <code>info</code>, <code>metadata</code>, <code>status</code>, <code>verify</code>, <code>clean</code>.</li> <li>pyEGA3-compatible credentials -- Same JSON format (<code>{\"username\": \"...\", \"password\": \"...\"}</code>).</li> <li>Single static binary -- No runtime dependencies; cross-compiled for Linux, macOS, and Windows.</li> </ul>"},{"location":"migration/","title":"Migrating from pyEGA3","text":"<p>EGAfetch is a drop-in replacement for pyEGA3 with a similar interface. This guide maps pyEGA3 commands to their EGAfetch equivalents.</p>"},{"location":"migration/#command-mapping","title":"Command Mapping","text":""},{"location":"migration/#authentication","title":"Authentication","text":"pyEGA3EGAfetch <pre><code># pyEGA3 reads credentials from -cf on every command\npyega3 -cf credentials.json fetch EGAD00001001938\n</code></pre> <pre><code># Option 1: Login once, then run commands\negafetch auth login --cf credentials.json\negafetch download EGAD00001001938\n\n# Option 2: Pass --cf to each command (same as pyEGA3)\negafetch download EGAD00001001938 --cf credentials.json\n</code></pre>"},{"location":"migration/#download-a-dataset","title":"Download a Dataset","text":"pyEGA3EGAfetch <pre><code>pyega3 -cf creds.json fetch EGAD00001001938 --output-dir ./data\n</code></pre> <pre><code>egafetch download EGAD00001001938 -o ./data --cf creds.json\n</code></pre>"},{"location":"migration/#download-specific-files","title":"Download Specific Files","text":"pyEGA3EGAfetch <pre><code>pyega3 -cf creds.json fetch EGAF00001104661\n</code></pre> <pre><code>egafetch download EGAF00001104661 --cf creds.json\n</code></pre>"},{"location":"migration/#list-dataset-files","title":"List Dataset Files","text":"pyEGA3EGAfetch <pre><code>pyega3 -cf creds.json files EGAD00001001938\n</code></pre> <pre><code>egafetch list EGAD00001001938 --cf creds.json\n</code></pre>"},{"location":"migration/#config-file","title":"Config File","text":"<p>The credentials file format is identical:</p> credentials.json<pre><code>{\n  \"username\": \"your.email@example.com\",\n  \"password\": \"your_password\"\n}\n</code></pre> <p>The only difference is the flag name: pyEGA3 uses <code>-cf</code>, EGAfetch uses <code>--cf</code> (double dash).</p>"},{"location":"migration/#key-differences","title":"Key Differences","text":"Feature pyEGA3 EGAfetch Parallelism Sequential (1 file, 1 stream) Configurable (4 files x 8 chunks default) Resume Re-downloads from scratch Byte-precise resume with HTTP Range Token refresh Fails when token expires Automatic refresh before expiry Progress Basic text output Live progress bars per file Interruption May corrupt state Safe at any point (atomic state writes) Metadata Not available <code>egafetch metadata</code> exports TSV/CSV/JSON Metadata auto-download Not available Auto-fetched after dataset download (with <code>--cf</code>) Bandwidth throttling Not available <code>--max-bandwidth</code> global limit File filtering Not available <code>--include</code> / <code>--exclude</code> glob patterns Adaptive chunk sizing Not available <code>--adaptive-chunks</code> auto-tunes based on throughput Persistent config Not available <code>~/.egafetch/config.yaml</code> for default settings Installation Python + pip Single binary, zero dependencies Batch file input Not available Text file with identifiers (one per line) MD5 sidecar files Not available <code>.md5</code> file written alongside each download <code>.cip</code> stripping Strips <code>.cip</code> extension Strips <code>.cip</code> extension (same behavior) Checksum After download After download (same, but automatic)"},{"location":"migration/#new-features-in-egafetch","title":"New Features in EGAfetch","text":"<p>Features not available in pyEGA3:</p> <ul> <li><code>egafetch metadata</code> -- Export dataset metadata as TSV, CSV, or JSON with a merged master file</li> <li><code>egafetch status</code> -- Check download progress without re-running the download</li> <li><code>egafetch verify</code> -- Re-verify checksums at any time</li> <li><code>egafetch clean</code> -- Remove temporary files while keeping completed downloads</li> <li><code>--restart</code> -- Force a fresh download, discarding all progress</li> <li><code>--parallel-files</code> / <code>--parallel-chunks</code> / <code>--chunk-size</code> -- Fine-grained control over download parallelism</li> <li><code>--max-bandwidth</code> -- Global bandwidth throttling (e.g., <code>100M</code>, <code>1G</code>)</li> <li><code>--include</code> / <code>--exclude</code> -- Glob-based file filtering (e.g., <code>--include \"*.bam\"</code>)</li> <li><code>--adaptive-chunks</code> -- Auto-adjust chunk sizes based on measured throughput</li> <li><code>--no-metadata</code> / <code>--metadata-format</code> -- Control automatic metadata download during dataset downloads</li> <li><code>~/.egafetch/config.yaml</code> -- Persistent config file for default settings (chunk size, parallelism, bandwidth, output dir)</li> <li>Identifier files -- Pass a text file with one EGAD/EGAF per line instead of listing IDs on the command line</li> <li>MD5 sidecar files -- <code>.md5</code> checksum file written alongside each downloaded file for easy verification</li> </ul>"},{"location":"architecture/overview/","title":"Architecture","text":""},{"location":"architecture/overview/#overview","title":"Overview","text":"<pre><code>CLI (cobra)\n    |\nOrchestrator              Manages file-level parallelism\n    |\nFileDownload              Per-file state machine\n    |\nChunkDownloader           HTTP Range requests + retries\n    |\nAuth Manager              Automatic token refresh\n</code></pre>"},{"location":"architecture/overview/#package-structure","title":"Package Structure","text":"<pre><code>egafetch/\n  cmd/egafetch/main.go       CLI entry point and command wiring\n  internal/\n    auth/\n      auth.go                 OAuth2 token management + refresh\n      credentials.go          Credential storage (~/.egafetch/)\n    api/\n      client.go               EGA API client (metadata + download)\n      types.go                API response types\n    config/\n      config.go               Persistent config file (~/.egafetch/config.yaml)\n    download/\n      orchestrator.go         Parallel file coordination\n      file.go                 Single file state machine + adaptive sizing\n      chunk.go                Chunk downloader with retries + throttling\n      merge.go                Chunk merging into final file\n    state/\n      manifest.go             Download manifest management\n      state.go                Per-file state persistence\n    verify/\n      checksum.go             MD5/SHA256 verification\n    ui/\n      progress.go             Terminal progress bars\n      status.go               Status display formatting\n</code></pre>"},{"location":"architecture/overview/#orchestrator","title":"Orchestrator","text":"<p>The orchestrator manages file-level parallelism using a semaphore pattern:</p> <ol> <li>Receives a manifest (list of files to download)</li> <li>Launches one goroutine per file</li> <li>Each goroutine checks if the file is already complete before acquiring a semaphore slot</li> <li>Up to <code>--parallel-files</code> (default 4) files download simultaneously</li> <li>Uses <code>errgroup</code> for cancellation propagation -- if one file fails fatally, all are cancelled</li> </ol>"},{"location":"architecture/overview/#file-state-machine","title":"File State Machine","text":"<p>Each file progresses through a deterministic state machine:</p> <pre><code>pending --&gt; chunking --&gt; downloading --&gt; merging --&gt; verifying --&gt; complete\n                            |              |\n                            v              v\n                          failed &lt;---------+\n</code></pre> State Description <code>pending</code> Initial state, chunks not yet created <code>chunking</code> Splitting file into chunk ranges <code>downloading</code> Actively downloading chunks in parallel <code>merging</code> Concatenating chunk files into the final output <code>verifying</code> Validating checksum (MD5/SHA256) and writing <code>.md5</code> sidecar <code>complete</code> Download successful, <code>.md5</code> written, chunks cleaned up <code>failed</code> Failed after retries; may be retried at file level <p>State is persisted to disk after every transition. This means you can interrupt at any point and resume cleanly.</p>"},{"location":"architecture/overview/#chunk-downloader","title":"Chunk Downloader","text":"<p>Files are split into chunks (default 64 MB) and downloaded in parallel:</p> <ol> <li>Each chunk is assigned a byte range (<code>start</code> to <code>end</code>)</li> <li>An HTTP Range request fetches exactly those bytes</li> <li>The response is streamed to a <code>.part</code> file</li> <li>If a <code>.part</code> file already has bytes on disk, the Range header starts from the existing size (resume)</li> <li>If the server returns HTTP 200 instead of 206 (ignoring the Range header), the existing file is truncated to prevent data corruption</li> <li>On completion, the chunk state is marked <code>complete</code></li> </ol> <p>Retry logic: Up to 5 retries per chunk with exponential backoff (1s base, 60s max) plus random jitter (0-1000ms).</p>"},{"location":"architecture/overview/#input-handling","title":"Input Handling","text":"<p>CLI arguments are processed through <code>expandArgs</code>, which supports three input types:</p> <ul> <li>Dataset IDs (<code>EGAD...</code>) -- expanded via the EGA API into all files in the dataset</li> <li>File IDs (<code>EGAF...</code>) -- fetched individually via the EGA metadata API</li> <li>Identifier files -- any argument not starting with <code>EGAD</code>/<code>EGAF</code> is read as a text file with one identifier per line (blank lines and <code>#</code> comments are ignored)</li> </ul>"},{"location":"architecture/overview/#disk-layout","title":"Disk Layout","text":"<pre><code>./output-dir/\n    .egafetch/\n        manifest.json              File list and dataset info\n        state/\n            EGAF00001104661.json   Per-file state (status, chunks, progress)\n        chunks/\n            EGAF00001104661/\n                000.part           Temporary chunk files\n                001.part\n                002.part\n    EGAF00001104661/\n        SLX-9630.A006.bwa.bam     Completed file (after merge + verify)\n        SLX-9630.A006.bwa.bam.md5 MD5 checksum sidecar (standard md5sum format)\n</code></pre> <p>After each file is downloaded, merged, and verified, an MD5 checksum file is written alongside the output file. This <code>.md5</code> file uses standard <code>md5sum</code> format and can be verified with <code>md5sum -c</code>.</p> <p>All JSON state files are written atomically (temp file + fsync + rename) to prevent corruption on crashes.</p>"},{"location":"architecture/overview/#authentication-flow","title":"Authentication Flow","text":"<p>EGAfetch uses two separate OAuth2 Identity Providers:</p> <p>Download API (<code>ega.ebi.ac.uk:8443</code>):</p> <ul> <li><code>grant_type=password</code> with EGA OIDC client credentials</li> <li>Tokens last ~1 hour</li> <li>Auto-refreshed 5 minutes before expiry using the refresh token</li> <li>Stored in <code>~/.egafetch/credentials.json</code></li> </ul> <p>Metadata API (<code>idp.ega-archive.org</code>):</p> <ul> <li>Separate IdP with <code>client_id=metadata-api</code></li> <li>Tokens last 300 seconds</li> <li>Not persisted (fetched on-demand for metadata commands)</li> </ul>"},{"location":"architecture/overview/#dependencies","title":"Dependencies","text":"Package Purpose <code>github.com/spf13/cobra</code> CLI framework <code>golang.org/x/sync</code> <code>errgroup</code> for goroutine coordination <code>golang.org/x/term</code> Hidden password input <code>golang.org/x/time</code> <code>rate.Limiter</code> for bandwidth throttling <code>gopkg.in/yaml.v3</code> YAML config file parsing"},{"location":"architecture/resume/","title":"Resume &amp; Recovery","text":"<p>EGAfetch is designed to be safe to interrupt at any point. This page explains how resume works at each level.</p>"},{"location":"architecture/resume/#how-resume-works","title":"How Resume Works","text":"<p>When you re-run a download command, EGAfetch checks existing state at three levels:</p>"},{"location":"architecture/resume/#1-file-level","title":"1. File Level","text":"<p>The orchestrator loads each file's state from <code>.egafetch/state/{fileID}.json</code> before acquiring a download slot:</p> <ul> <li><code>complete</code> -- file is skipped instantly (no network call)</li> <li><code>downloading</code> / <code>merging</code> / <code>verifying</code> -- file is resumed from its current state</li> <li><code>failed</code> -- file is retried (up to 3 times)</li> <li><code>pending</code> -- file starts fresh</li> </ul>"},{"location":"architecture/resume/#2-chunk-level","title":"2. Chunk Level","text":"<p>Within a file, only incomplete chunks are processed. The file state machine calls <code>PendingChunks()</code> which returns chunks not in the <code>complete</code> state.</p> <p>For each pending chunk, the downloader checks the <code>.part</code> file on disk:</p> <ul> <li>If the <code>.part</code> file has <code>N</code> bytes already, the HTTP Range request starts at byte <code>chunk.Start + N</code></li> <li>If the <code>.part</code> file is complete (size matches expected), the chunk is marked complete without a network call</li> </ul>"},{"location":"architecture/resume/#3-byte-level","title":"3. Byte Level","text":"<p>HTTP Range requests resume from the exact byte where the previous download stopped:</p> <pre><code>Range: bytes=1048576-2097151\n</code></pre> <p>The <code>.part</code> file is opened in append mode (<code>O_APPEND</code>), so new bytes are added after existing content.</p>"},{"location":"architecture/resume/#server-response-handling","title":"Server Response Handling","text":"<p>When resuming a partial chunk, the server should respond with HTTP 206 (Partial Content). However, some servers may ignore the <code>Range</code> header and return HTTP 200 (OK) with the full content. EGAfetch detects this case and automatically truncates the existing <code>.part</code> file before writing, preventing data corruption from appending full content to partial data.</p>"},{"location":"architecture/resume/#state-persistence","title":"State Persistence","text":"<p>State is saved to disk at critical points:</p> Event What's Saved File state transition <code>{fileID}.json</code> updated with new status Chunk completion <code>{fileID}.json</code> updated with chunk marked <code>complete</code> Download start Manifest saved to <code>manifest.json</code> Graceful shutdown (Ctrl+C) In-progress state preserved as-is <p>All writes are atomic (write to temp file, fsync, rename). This prevents corruption if the process is killed during a write.</p>"},{"location":"architecture/resume/#scenarios","title":"Scenarios","text":""},{"location":"architecture/resume/#network-failure-mid-download","title":"Network Failure Mid-Download","text":"<pre><code>egafetch download EGAD00001001938 -o ./data\n# Network goes down after 30 files complete, 2 files partially downloaded\n\negafetch download EGAD00001001938 -o ./data\n# 30 files skipped, 2 files resume from partial state, remaining files download\n</code></pre>"},{"location":"architecture/resume/#ctrlc-during-download","title":"Ctrl+C During Download","text":"<pre><code>egafetch download EGAD00001001938 -o ./data\n# ^C pressed -- \"Interrupted. Saving state...\"\n\negafetch download EGAD00001001938 -o ./data\n# Resumes from exactly where it stopped\n</code></pre>"},{"location":"architecture/resume/#process-kill-sigkill-oom","title":"Process Kill (SIGKILL / OOM)","text":"<p>Even without graceful shutdown, resume works because:</p> <ul> <li>Chunk <code>.part</code> files are written incrementally</li> <li>State files are written atomically (no partial JSON)</li> <li>The chunk downloader detects existing bytes on disk via file size</li> </ul> <p>The worst case is re-downloading the bytes written since the last state save (at most one chunk's worth of data).</p>"},{"location":"architecture/resume/#force-fresh-start","title":"Force Fresh Start","text":"<pre><code>egafetch download EGAD00001001938 -o ./data --restart\n# Removes .egafetch/ directory and starts from scratch\n</code></pre> <p>Warning</p> <p><code>--restart</code> deletes all download state including partial chunks. Completed files in the output directory are not deleted, but they will be re-downloaded and overwritten.</p>"},{"location":"architecture/resume/#md5-checksum-files","title":"MD5 Checksum Files","text":"<p>After a file passes verification, EGAfetch computes its MD5 checksum and writes a <code>.md5</code> sidecar file alongside the output file. This file uses standard <code>md5sum</code> format and is generated regardless of whether the EGA API provided a checksum. On resume, if the file is already in the <code>complete</code> state (with its <code>.md5</code> file already written), it is skipped entirely.</p>"},{"location":"architecture/resume/#idempotency","title":"Idempotency","text":"<p>Running the same download command multiple times is idempotent:</p> <ul> <li>Complete files are skipped (including their <code>.md5</code> sidecar files)</li> <li>The manifest is overwritten with the same content</li> <li>No data is duplicated or corrupted</li> <li>Checksums are verified before marking any file complete</li> </ul>"},{"location":"commands/auth/","title":"Authentication","text":"<p>Manage your EGA session with the <code>auth</code> subcommands.</p>"},{"location":"commands/auth/#login","title":"Login","text":"<pre><code>egafetch auth login [--cf FILE]\n</code></pre> <p>Authenticates with EGA and stores OAuth2 tokens locally.</p> <p>Interactive mode (default):</p> <pre><code>egafetch auth login\n# EGA Username (email): user@example.com\n# EGA Password: ********\n# Authenticating...\n# Login successful!\n</code></pre> <p>Config file mode:</p> <pre><code>egafetch auth login --cf credentials.json\n# Authenticating...\n# Login successful!\n</code></pre>"},{"location":"commands/auth/#flags","title":"Flags","text":"Flag Description <code>--cf</code> Path to JSON config file (<code>{\"username\":\"...\",\"password\":\"...\"}</code>) <code>--config-file</code> Alias for <code>--cf</code>"},{"location":"commands/auth/#status","title":"Status","text":"<pre><code>egafetch auth status\n</code></pre> <p>Shows the current authentication state without refreshing the token.</p> <p>Output when logged in:</p> <pre><code>Logged in as: user@example.com\nToken expires: 58m30s\n</code></pre> <p>Output when not logged in:</p> <pre><code>Not logged in. Run 'egafetch auth login' to authenticate.\n</code></pre>"},{"location":"commands/auth/#logout","title":"Logout","text":"<pre><code>egafetch auth logout\n</code></pre> <p>Clears stored credentials from both memory and disk (<code>~/.egafetch/credentials.json</code>).</p> <pre><code>Logged out.\n</code></pre>"},{"location":"commands/download/","title":"Download","text":"<p>Download datasets or individual files from EGA with parallel chunked downloads and automatic resume.</p>"},{"location":"commands/download/#usage","title":"Usage","text":"<pre><code>egafetch download [EGAD.../EGAF.../file.txt] [flags]\n</code></pre> <p>Arguments can be dataset IDs (<code>EGAD...</code>), file IDs (<code>EGAF...</code>), or text files containing one identifier per line. See Identifier Files below.</p>"},{"location":"commands/download/#examples","title":"Examples","text":"<pre><code># Download all files in a dataset\negafetch download EGAD00001001938 -o ./data\n\n# Download specific files\negafetch download EGAF00001104661 EGAF00001104662 -o ./data\n\n# Download from a list of identifiers in a text file\negafetch download identifiers.txt -o ./data --cf credentials.json\n\n# Mix direct IDs and identifier files\negafetch download EGAF00000009999 identifiers.txt -o ./data\n\n# Tune parallelism for fast networks\negafetch download EGAD00001001938 -o ./data \\\n    --parallel-files 8 \\\n    --parallel-chunks 16 \\\n    --chunk-size 128M\n\n# Limit bandwidth on shared HPC networks\negafetch download EGAD00001001938 -o ./data --max-bandwidth 100M\n\n# Download only BAM files, excluding unmapped\negafetch download EGAD00001001938 -o ./data \\\n    --include \"*.bam\" \\\n    --exclude \"*_unmapped*\"\n\n# Auto-tune chunk sizes based on connection speed\negafetch download EGAD00001001938 -o ./data --adaptive-chunks\n\n# Force a completely fresh download\negafetch download EGAD00001001938 -o ./data --restart\n\n# Non-interactive (for scripts / HPC jobs)\negafetch download EGAD00001001938 -o ./data --cf credentials.json\n</code></pre>"},{"location":"commands/download/#flags","title":"Flags","text":"Flag Default Description <code>-o, --output</code> <code>.</code> Output directory for downloaded files <code>--parallel-files</code> <code>4</code> Number of files downloaded simultaneously <code>--parallel-chunks</code> <code>8</code> Number of chunks per file downloaded simultaneously <code>--chunk-size</code> <code>64M</code> Size of each chunk (supports <code>K</code>, <code>M</code>, <code>G</code> suffixes) <code>--max-bandwidth</code> Global bandwidth limit (e.g., <code>100M</code>, <code>1G</code>) <code>--include</code> Glob patterns to include (matched against file name) <code>--exclude</code> Glob patterns to exclude (matched against file name) <code>--adaptive-chunks</code> <code>false</code> Auto-adjust chunk size based on throughput <code>--no-metadata</code> <code>false</code> Skip downloading dataset metadata <code>--metadata-format</code> <code>tsv</code> Metadata output format (<code>tsv</code>, <code>csv</code>, <code>json</code>) <code>--restart</code> <code>false</code> Wipe all existing progress and start fresh <code>--cf, --config-file</code> JSON config file with credentials"},{"location":"commands/download/#automatic-resume","title":"Automatic Resume","text":"<p>Re-running the same download command automatically resumes where it left off:</p> <ul> <li>Completed files are skipped instantly (no network call)</li> <li>Partial files resume from the last downloaded byte using HTTP Range requests</li> <li>Failed files are retried (up to 3 attempts per file)</li> </ul> <pre><code># First run -- downloads 30 of 60 files, then interrupted\negafetch download EGAD00001001938 -o ./data\n# ^C\n\n# Second run -- skips the 30 completed files, resumes the rest\negafetch download EGAD00001001938 -o ./data\n</code></pre> <p>No separate <code>resume</code> command is needed.</p>"},{"location":"commands/download/#fresh-start","title":"Fresh Start","text":"<p>If you want to discard all progress and re-download everything:</p> <pre><code>egafetch download EGAD00001001938 -o ./data --restart\n</code></pre> <p>This removes the <code>.egafetch/</code> state directory before proceeding.</p>"},{"location":"commands/download/#tuning-performance","title":"Tuning Performance","text":""},{"location":"commands/download/#parallel-files","title":"Parallel Files","text":"<p>Controls how many files are downloaded at the same time. Increase this if you have many small files:</p> <pre><code>--parallel-files 8\n</code></pre>"},{"location":"commands/download/#parallel-chunks","title":"Parallel Chunks","text":"<p>Controls how many chunks of a single file are downloaded simultaneously. Increase for large files on fast networks:</p> <pre><code>--parallel-chunks 16\n</code></pre>"},{"location":"commands/download/#chunk-size","title":"Chunk Size","text":"<p>Controls the size of each chunk. Larger chunks mean fewer HTTP requests but coarser resume granularity:</p> <pre><code>--chunk-size 128M   # Good for fast, stable connections\n--chunk-size 32M    # Good for unstable connections (finer resume)\n</code></pre>"},{"location":"commands/download/#bandwidth-throttling","title":"Bandwidth Throttling","text":"<p>Cap the total download bandwidth across all parallel connections. Useful on shared HPC networks where you should not saturate the link:</p> <pre><code>--max-bandwidth 100M   # Limit to 100 MB/s total\n--max-bandwidth 1G     # Limit to 1 GB/s total\n</code></pre> <p>The limit is enforced globally -- all files and chunks share the same bandwidth pool.</p>"},{"location":"commands/download/#adaptive-chunk-sizing","title":"Adaptive Chunk Sizing","text":"<p>When enabled, EGAfetch monitors download throughput and automatically adjusts chunk sizes:</p> <pre><code>egafetch download EGAD00001001938 -o ./data --adaptive-chunks\n</code></pre> <ul> <li>Chunks start at the default size (64 MB or <code>--chunk-size</code> value)</li> <li>Throughput is measured over a rolling window of 3 chunks</li> <li>Fast connections (&gt; 50 MB/s avg): chunk size scaled up by 1.5x (max 256 MB)</li> <li>Slow connections (&lt; 10 MB/s avg): chunk size scaled down by 0.5x (min 8 MB)</li> </ul> <p>This is useful when you don't know the network speed in advance. Larger chunks reduce HTTP overhead on fast links, while smaller chunks reduce wasted work on failures with slow links.</p>"},{"location":"commands/download/#file-filtering","title":"File Filtering","text":"<p>Selectively download files from a dataset using glob patterns:</p> <pre><code># Only BAM files\negafetch download EGAD00001001938 -o ./data --include \"*.bam\"\n\n# Everything except encrypted files\negafetch download EGAD00001001938 -o ./data --exclude \"*.cip\"\n\n# Combine: only BAM files, but not unmapped ones\negafetch download EGAD00001001938 -o ./data \\\n    --include \"*.bam\" \\\n    --exclude \"*_unmapped*\"\n</code></pre> <ul> <li>Patterns are matched against the file name only (not the full path)</li> <li>Uses Go's <code>filepath.Match</code> syntax (<code>*</code>, <code>?</code>, <code>[...]</code>)</li> <li><code>--include</code>: file must match at least one include pattern</li> <li><code>--exclude</code>: file is skipped if it matches any exclude pattern</li> <li>Explicitly named EGAF file IDs are never filtered out</li> <li>Multiple patterns can be specified by repeating the flag</li> </ul>"},{"location":"commands/download/#metadata-during-download","title":"Metadata During Download","text":"<p>When downloading a dataset (EGAD) with <code>--cf</code>, metadata is fetched automatically after the data download completes. Use <code>--no-metadata</code> to skip, or <code>--metadata-format</code> to choose the format:</p> <pre><code># Download data + metadata as TSV (default)\negafetch download EGAD00001001938 -o ./data --cf creds.json\n\n# Download data + metadata as JSON\negafetch download EGAD00001001938 -o ./data --cf creds.json --metadata-format json\n\n# Download data only, skip metadata\negafetch download EGAD00001001938 -o ./data --cf creds.json --no-metadata\n</code></pre> <p>Metadata files are saved to <code>{output}/{datasetID}-metadata/</code>. If metadata auth fails, the data download still succeeds with a warning.</p>"},{"location":"commands/download/#identifier-files","title":"Identifier Files","text":"<p>Any argument that does not start with <code>EGAD</code> or <code>EGAF</code> is treated as a text file containing identifiers, one per line. This is useful for batch downloads from curated lists:</p> identifiers.txt<pre><code># WGS samples from project X\nEGAD00001002071\n\n# Additional individual files\nEGAF00000001234\nEGAF00000005678\n</code></pre> <pre><code>egafetch download identifiers.txt -o ./data --cf credentials.json\n</code></pre> <ul> <li>Blank lines and lines starting with <code>#</code> are ignored</li> <li>Mixed <code>EGAD</code> and <code>EGAF</code> identifiers are allowed in the same file</li> <li>You can combine identifier files with direct IDs on the command line</li> <li>Errors include the filename and line number for easy debugging</li> </ul>"},{"location":"commands/download/#output-file-names","title":"Output File Names","text":"<p>EGA stores files in encrypted <code>.cip</code> format. When downloading in plain (decrypted) mode (the default), EGAfetch automatically strips the <code>.cip</code> extension from output file names. For example, <code>sample.bam.cip</code> on the EGA server becomes <code>sample.bam</code> in your output directory.</p>"},{"location":"commands/download/#md5-checksum-files","title":"MD5 Checksum Files","text":"<p>After each file is downloaded and verified, EGAfetch writes an MD5 checksum sidecar file alongside the downloaded file. For example:</p> <pre><code>output-dir/\n    EGAF00001104661/\n        SLX-9630.A006.bwa.bam       # Downloaded file\n        SLX-9630.A006.bwa.bam.md5   # MD5 checksum file\n</code></pre> <p>The <code>.md5</code> file uses standard <code>md5sum</code> format:</p> <pre><code>a1b2c3d4e5f6...  SLX-9630.A006.bwa.bam\n</code></pre> <p>This allows verification with standard tools: <code>cd output-dir/EGAF00001104661 &amp;&amp; md5sum -c SLX-9630.A006.bwa.bam.md5</code>.</p>"},{"location":"commands/download/#recommended-settings","title":"Recommended Settings","text":"Scenario Flags HPC with fast network <code>--parallel-files 8 --parallel-chunks 16 --chunk-size 128M</code> HPC with shared link <code>--parallel-files 4 --max-bandwidth 500M</code> Laptop on WiFi <code>--parallel-files 2 --parallel-chunks 4 --chunk-size 32M</code> Unknown network <code>--adaptive-chunks</code> Many small files <code>--parallel-files 16 --parallel-chunks 4</code> Few large files <code>--parallel-files 2 --parallel-chunks 16 --chunk-size 128M</code> Only BAM files <code>--include \"*.bam\"</code>"},{"location":"commands/download/#progress-output","title":"Progress Output","text":"<p>During download, a live progress display shows the state of each file:</p> <pre><code>Downloading 60 file(s) to ./data\n  SLX-9630.A006.bwa.bam  [========&gt;         ] 45%  225.0 MB / 500.0 MB\n  SLX-9630.A007.bwa.bam  [==============&gt;   ] 72%  230.4 MB / 320.0 MB\n  SLX-9631.A001.bwa.bam  [=&gt;                ]  8%   12.0 MB / 150.0 MB\n  SLX-9631.A002.bwa.bam  [waiting...]\n</code></pre> <p>Status indicators:</p> Display Meaning <code>[========&gt;    ] 45%</code> Actively downloading <code>[========================] 100%</code> Complete <code>[---- skipped ----]</code> Already complete from a previous run <code>[---- FAILED  ----]</code> Download failed after retries <code>[waiting...]</code> Queued, waiting for a parallel slot"},{"location":"commands/download/#retry-behavior","title":"Retry Behavior","text":"<p>EGAfetch automatically retries on transient errors:</p> <ul> <li>Per chunk: Up to 5 retries with exponential backoff (1s, 2s, 4s, 8s, 16s) plus random jitter, capped at 60 seconds</li> <li>Per file: Up to 3 retries of the entire file state machine</li> <li>Retryable errors: Network timeouts, connection resets, HTTP 5xx, HTTP 429 (rate limited)</li> <li>Non-retryable errors: HTTP 4xx (except 429), authentication failures</li> </ul>"},{"location":"commands/download/#graceful-interruption","title":"Graceful Interruption","text":"<p>Pressing <code>Ctrl+C</code> triggers a graceful shutdown:</p> <ol> <li>In-flight HTTP requests are cancelled</li> <li>Current state is saved to disk</li> <li>Partial chunk files are preserved for resume</li> </ol> <p>You can safely interrupt at any time without data loss.</p>"},{"location":"commands/info/","title":"Dataset &amp; File Info","text":""},{"location":"commands/info/#list-authorized-datasets","title":"List Authorized Datasets","text":"<pre><code>egafetch list [--cf FILE]\n</code></pre> <p>When run without arguments, lists all datasets the authenticated user has access to.</p> <pre><code>egafetch list --cf credentials.json\n</code></pre> <pre><code>Fetching authorized datasets...\n\nAuthorized datasets (3):\n\n  EGAD00001001938\n  EGAD00001003245\n  EGAD00001005678\n</code></pre>"},{"location":"commands/info/#list-files-in-a-dataset","title":"List Files in a Dataset","text":"<pre><code>egafetch list EGAD... [--cf FILE]\n</code></pre> <p>When a dataset ID is provided, lists all files in that dataset with their IDs, sizes, and checksums.</p> <pre><code>egafetch list EGAD00001001938\n</code></pre> <pre><code>Fetching files for dataset EGAD00001001938...\nFile ID              Size         Check  File Name\n---------------------------------------------------------------------------\nEGAF00001104661     500.0 MB     MD5    SLX-9630.A006.bwa.bam\nEGAF00001104662     320.0 MB     MD5    SLX-9630.A007.bwa.bam\nEGAF00001104480     450.0 MB     MD5    SLX-9630.A005.bwa.bam\n...\n\n60 files, 25.3 GB total\n</code></pre>"},{"location":"commands/info/#flags","title":"Flags","text":"Flag Description <code>--cf, --config-file</code> JSON config file with credentials"},{"location":"commands/info/#show-file-metadata","title":"Show File Metadata","text":"<pre><code>egafetch info EGAF... [--cf FILE]\n</code></pre> <p>Displays detailed metadata for a single file.</p> <pre><code>egafetch info EGAF00001104661\n</code></pre> <pre><code>File ID:       EGAF00001104661\nFile Name:     SLX-9630.A006.bwa.bam\nFile Size:     500.0 MB (524288000 bytes)\nChecksum:      d41d8cd98f00b204e9800998ecf8427e\nChecksum Type: MD5\nStatus:        available\n</code></pre>"},{"location":"commands/info/#flags_1","title":"Flags","text":"Flag Description <code>--cf, --config-file</code> JSON config file with credentials"},{"location":"commands/management/","title":"Management Commands","text":""},{"location":"commands/management/#status","title":"Status","text":"<pre><code>egafetch status [directory]\n</code></pre> <p>Shows the download progress for all files tracked in the given directory.</p> <pre><code>egafetch status ./data\n</code></pre> <pre><code>File ID              Status          Size         Progress  File Name\n--------------------------------------------------------------------------------\nEGAF00001104661     complete        500.0 MB     100%      SLX-9630.A006.bwa.bam\nEGAF00001104662     downloading     320.0 MB     45.3%     SLX-9630.A007.bwa.bam\nEGAF00001104480     pending         450.0 MB     -         SLX-9630.A005.bwa.bam\n</code></pre> <p>Default directory is <code>.</code> (current directory).</p>"},{"location":"commands/management/#verify","title":"Verify","text":"<pre><code>egafetch verify [directory]\n</code></pre> <p>Re-verifies checksums (MD5 or SHA256) of all completed files.</p> <pre><code>egafetch verify ./data\n</code></pre> <pre><code>  OK    SLX-9630.A006.bwa.bam\n  OK    SLX-9630.A007.bwa.bam\n  SKIP  SLX-9630.A005.bwa.bam (status: downloading)\n\n2 passed, 0 failed, 1 skipped\n</code></pre> Status Meaning <code>OK</code> Checksum matches expected value <code>FAIL</code> Checksum mismatch -- file may be corrupted <code>SKIP</code> File not yet complete, or no checksum available <p>If any files fail verification, the command exits with a non-zero status code.</p>"},{"location":"commands/management/#clean","title":"Clean","text":"<pre><code>egafetch clean [directory]\n</code></pre> <p>Removes temporary files while keeping completed downloads:</p> <ul> <li>Deletes all chunk files (<code>.egafetch/chunks/</code>)</li> <li>Removes state files for completed downloads</li> <li>Keeps state files for incomplete downloads (so they can still resume)</li> </ul> <pre><code>egafetch clean ./data\n</code></pre> <pre><code>Removing chunk files from ./data/.egafetch/chunks/...\nCleaned 45 completed state file(s).\n</code></pre> <p>Note</p> <p><code>clean</code> does not remove the final downloaded files -- only the temporary chunks and state tracking files.</p>"},{"location":"commands/metadata/","title":"Metadata Export","text":"<p>Download dataset metadata from the EGA Private Metadata API and export as TSV, CSV, or JSON.</p> <p>Auto-download during <code>egafetch download</code></p> <p>When downloading a dataset with <code>--cf</code>, metadata is fetched automatically after the data download completes. You only need the standalone <code>metadata</code> command if you want metadata without downloading files, or need to re-fetch metadata separately.</p>"},{"location":"commands/metadata/#usage","title":"Usage","text":"<pre><code>egafetch metadata EGAD... [flags]\n</code></pre>"},{"location":"commands/metadata/#examples","title":"Examples","text":"<pre><code># Export as TSV (default)\negafetch metadata EGAD00001001938\n\n# Export as CSV\negafetch metadata EGAD00001001938 --format csv\n\n# Export as JSON to a custom directory\negafetch metadata EGAD00001001938 --format json -o ./my-metadata\n\n# Non-interactive with config file\negafetch metadata EGAD00001001938 --cf credentials.json\n</code></pre>"},{"location":"commands/metadata/#flags","title":"Flags","text":"Flag Default Description <code>-f, --format</code> <code>tsv</code> Output format: <code>tsv</code>, <code>csv</code>, or <code>json</code> <code>-o, --output</code> <code>{datasetID}-metadata</code> Output directory <code>--cf, --config-file</code> JSON config file with credentials"},{"location":"commands/metadata/#output-files","title":"Output Files","text":"<p>The command creates a directory with individual mapping files plus a merged file:</p> <pre><code>EGAD00001001938-metadata/\n    study_experiment_run_sample.tsv    # Study/experiment/run/sample mappings\n    run_sample.tsv                     # Run-to-sample mappings\n    study_analysis_sample.tsv          # Study/analysis/sample mappings\n    analysis_sample.tsv                # Analysis-to-sample mappings\n    sample_file.tsv                    # Sample-to-file mappings\n    EGAD00001001938_merged_metadata.tsv # Merged main file\n</code></pre>"},{"location":"commands/metadata/#merged-metadata-file","title":"Merged Metadata File","text":"<p>This file merges <code>study_experiment_run_sample</code> with <code>sample_file</code> on the <code>sample_accession_id</code> column. This gives you a single wide table linking studies, experiments, runs, samples, and files.</p> <p>If a column name exists in both tables, the <code>sample_file</code> column is prefixed with <code>file_</code> to avoid collisions.</p>"},{"location":"commands/metadata/#authentication","title":"Authentication","text":"<p>The metadata API uses a separate Identity Provider from the download API. This means:</p> <ul> <li>With <code>--cf</code>: Credentials are read from the file -- no prompts</li> <li>Without <code>--cf</code>: You must be logged in (<code>egafetch auth login</code> first), and you will be prompted for your password</li> </ul> <p>The metadata token is short-lived (300 seconds) but the entire metadata fetch completes well within that window.</p>"},{"location":"commands/metadata/#ega-mapping-endpoints","title":"EGA Mapping Endpoints","text":"<p>EGAfetch fetches from these EGA Private Metadata API endpoints:</p> Mapping Description <code>study_experiment_run_sample</code> Links studies to experiments, runs, and samples <code>run_sample</code> Maps sequencing runs to samples <code>study_analysis_sample</code> Links studies to analyses and samples <code>analysis_sample</code> Maps analyses to samples <code>sample_file</code> Maps samples to their EGA file accessions and filenames"},{"location":"getting-started/configuration/","title":"Configuration","text":""},{"location":"getting-started/configuration/#config-file","title":"Config File","text":"<p>Persist default download settings in <code>~/.egafetch/config.yaml</code> so you don't have to repeat flags every time. CLI flags always override config file values.</p> ~/.egafetch/config.yaml<pre><code>chunk_size: 128M\nparallel_files: 4\nparallel_chunks: 8\nmax_bandwidth: 500M\noutput_dir: /data/ega\nmetadata_format: tsv\n</code></pre> <p>All fields are optional. If the file doesn't exist, hardcoded defaults are used. The precedence is:</p> <ol> <li>CLI flags (highest priority)</li> <li>Config file (<code>~/.egafetch/config.yaml</code>)</li> <li>Hardcoded defaults (lowest priority)</li> </ol>"},{"location":"getting-started/configuration/#available-settings","title":"Available Settings","text":"Setting Equivalent Flag Default Description <code>chunk_size</code> <code>--chunk-size</code> <code>64M</code> Size of each download chunk <code>parallel_files</code> <code>--parallel-files</code> <code>4</code> Files downloaded simultaneously <code>parallel_chunks</code> <code>--parallel-chunks</code> <code>8</code> Chunks per file downloaded simultaneously <code>max_bandwidth</code> <code>--max-bandwidth</code> Global bandwidth limit <code>output_dir</code> <code>-o, --output</code> <code>.</code> Default output directory <code>metadata_format</code> <code>--metadata-format</code> <code>tsv</code> Default metadata format"},{"location":"getting-started/configuration/#credentials-file","title":"Credentials File","text":"<p>EGAfetch supports a JSON config file compatible with pyEGA3's <code>-cf</code> format:</p> credentials.json<pre><code>{\n  \"username\": \"your.email@example.com\",\n  \"password\": \"your_password\"\n}\n</code></pre> <p>Use it with any command via <code>--cf</code> or <code>--config-file</code>:</p> <pre><code>egafetch auth login --cf credentials.json\negafetch download EGAD00001001938 --cf credentials.json\negafetch metadata EGAD00001001938 --cf credentials.json\n</code></pre> <p>Keep your credentials safe</p> <p>Add <code>credentials.json</code> (or whatever you name it) to your <code>.gitignore</code>. The file contains your password in plain text.</p>"},{"location":"getting-started/configuration/#stored-session","title":"Stored Session","text":"<p>After logging in, EGAfetch stores your OAuth2 tokens at:</p> <pre><code>~/.egafetch/credentials.json\n</code></pre> <p>This file has <code>0600</code> permissions (owner read/write only) and contains:</p> <pre><code>{\n  \"username\": \"your.email@example.com\",\n  \"access_token\": \"eyJhbGc...\",\n  \"refresh_token\": \"eyJhbGc...\",\n  \"expires_at\": \"2025-02-10T14:30:00Z\"\n}\n</code></pre> <p>Tokens are automatically refreshed 5 minutes before expiry. You do not need to re-login between downloads unless the refresh token itself has expired.</p>"},{"location":"getting-started/configuration/#commands-that-accept-cf","title":"Commands That Accept <code>--cf</code>","text":"Command Effect <code>auth login</code> Read credentials from file instead of prompting <code>download</code> Auto-login before downloading <code>list</code> Auto-login before listing dataset files <code>info</code> Auto-login before fetching file metadata <code>metadata</code> Auto-login + use password for metadata API <p>When <code>--cf</code> is passed to a non-auth command, EGAfetch performs a fresh login before executing the command. This is useful for long-running jobs where a previous session may have expired.</p>"},{"location":"getting-started/configuration/#token-lifetimes","title":"Token Lifetimes","text":"<p>Token lifetimes are set by EGA's servers and cannot be changed client-side:</p> Token Lifetime Refresh Download API ~1 hour Automatic via refresh token Metadata API 300 seconds Not needed (quick operation) <p>EGAfetch handles refresh transparently. For the download API, tokens are refreshed 5 minutes before expiry using the refresh token. The metadata API token is short-lived but the metadata fetch completes well within 5 minutes.</p>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#from-source","title":"From Source","text":"<p>Requires Go 1.22+.</p> <pre><code>git clone https://github.com/khan-lab/EGAfetch.git\ncd EGAfetch\nmake build\n</code></pre> <p>The binary is built to <code>./bin/egafetch</code>. Copy it to a directory in your <code>$PATH</code>:</p> <pre><code>cp ./bin/egafetch /usr/local/bin/\n</code></pre> <p>Or install directly to <code>$GOPATH/bin</code>:</p> <pre><code>make install\n</code></pre>"},{"location":"getting-started/installation/#pre-built-binaries","title":"Pre-built Binaries","text":"<p>Download the latest release for your platform from the Releases page.</p> Platform Binary Linux (x86_64) <code>egafetch-linux-amd64</code> Linux (ARM64) <code>egafetch-linux-arm64</code> macOS (Intel) <code>egafetch-darwin-amd64</code> macOS (Apple Silicon) <code>egafetch-darwin-arm64</code> Windows (x86_64) <code>egafetch-windows-amd64.exe</code> <p>After downloading:</p> <pre><code>chmod +x egafetch-linux-amd64\nmv egafetch-linux-amd64 /usr/local/bin/egafetch\n</code></pre>"},{"location":"getting-started/installation/#cross-compile-all-platforms","title":"Cross-Compile All Platforms","text":"<pre><code>make release\nls bin/\n# egafetch-linux-amd64\n# egafetch-linux-arm64\n# egafetch-darwin-amd64\n# egafetch-darwin-arm64\n# egafetch-windows-amd64.exe\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code>egafetch --version\n</code></pre>"},{"location":"getting-started/installation/#hpc-clusters","title":"HPC Clusters","text":"<p>EGAfetch is a statically-linked Go binary with zero runtime dependencies. Copy the single binary to your cluster -- no modules, conda environments, or pip installs needed.</p> <pre><code>scp ./bin/egafetch user@cluster:/home/user/bin/\n</code></pre>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>This guide walks you through your first download with EGAfetch.</p>"},{"location":"getting-started/quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>An EGA account with access to at least one dataset</li> <li>EGAfetch binary installed (see Installation)</li> </ul>"},{"location":"getting-started/quickstart/#step-1-log-in","title":"Step 1: Log In","text":"InteractiveConfig File <pre><code>egafetch auth login\n</code></pre> <p>You will be prompted for your EGA email and password (password is hidden).</p> <p>Create a JSON file with your credentials:</p> credentials.json<pre><code>{\n  \"username\": \"your.email@example.com\",\n  \"password\": \"your_password\"\n}\n</code></pre> <p>Then log in:</p> <pre><code>egafetch auth login --cf credentials.json\n</code></pre> <p>Verify your session:</p> <pre><code>egafetch auth status\n# Logged in as: your.email@example.com\n# Token expires: 58m30s\n</code></pre>"},{"location":"getting-started/quickstart/#step-2-explore-a-dataset","title":"Step 2: Explore a Dataset","text":"<p>List the files in a dataset:</p> <pre><code>egafetch list EGAD00001001938\n</code></pre> <p>Output:</p> <pre><code>File ID              Size         Check  File Name\n---------------------------------------------------------------------------\nEGAF00001104661     500.0 MB     MD5    SLX-9630.A006.bwa.bam\nEGAF00001104662     320.0 MB     MD5    SLX-9630.A007.bwa.bam\n...\n\n60 files, 25.3 GB total\n</code></pre>"},{"location":"getting-started/quickstart/#step-3-download","title":"Step 3: Download","text":"<p>Download the entire dataset:</p> <pre><code>egafetch download EGAD00001001938 -o ./my-data\n</code></pre> <p>Or download specific files:</p> <pre><code>egafetch download EGAF00001104661 EGAF00001104662 -o ./my-data\n</code></pre> <p>Or download from a text file with identifiers (one per line, <code>#</code> comments allowed):</p> <pre><code>egafetch download identifiers.txt -o ./my-data --cf credentials.json\n</code></pre> <p>You will see live progress for each file:</p> <pre><code>Downloading 60 file(s) to ./my-data\n  SLX-9630.A006.bwa.bam  [========&gt;         ] 45%  225.0 MB / 500.0 MB\n  SLX-9630.A007.bwa.bam  [====&gt;             ] 22%   70.4 MB / 320.0 MB\n  SLX-9631.A001.bwa.bam  [waiting...]\n</code></pre>"},{"location":"getting-started/quickstart/#step-4-resume-if-interrupted","title":"Step 4: Resume (If Interrupted)","text":"<p>If the download is interrupted (Ctrl+C, network failure, etc.), simply re-run the same command:</p> <pre><code>egafetch download EGAD00001001938 -o ./my-data\n</code></pre> <p>Completed files are skipped, partial files resume from the last byte.</p>"},{"location":"getting-started/quickstart/#step-5-verify","title":"Step 5: Verify","text":"<p>Re-verify checksums of all completed files:</p> <pre><code>egafetch verify ./my-data\n</code></pre>"},{"location":"getting-started/quickstart/#step-6-clean-up","title":"Step 6: Clean Up","text":"<p>Remove temporary chunk files (keeps your completed downloads):</p> <pre><code>egafetch clean ./my-data\n</code></pre>"},{"location":"getting-started/quickstart/#using-a-config-file-everywhere","title":"Using a Config File Everywhere","text":"<p>Pass <code>--cf</code> to any command to skip interactive login:</p> <pre><code>egafetch download EGAD00001001938 -o ./data --cf credentials.json\negafetch list EGAD00001001938 --cf credentials.json\negafetch metadata EGAD00001001938 --cf credentials.json\n</code></pre> <p>This is especially useful for scripts and batch jobs on HPC clusters.</p>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Download command reference -- all flags and tuning options</li> <li>Metadata export -- export dataset metadata as TSV/CSV/JSON</li> <li>Configuration -- credentials and config file details</li> </ul>"}]}